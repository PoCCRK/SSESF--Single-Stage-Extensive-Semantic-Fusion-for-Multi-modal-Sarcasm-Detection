Namespace(aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=16, captioning_mask_prob=0.6, checkpoint_activations=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=0, cutmix_minmax=None, data_path='/hy-tmp/data/data-of-multimodal-sarcasm-detection/text', device='cuda', dist_eval=False, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, drop_worst_after=12000, drop_worst_ratio=0.2, enable_deepspeed=False, epochs=20, eval=True, eval_batch_size=None, finetune='/hy-tmp/models/best_state/checkpoint-best.pth', initial_scale_power=16, input_size=224, label_smoothing=0.1, layer_decay=0.9, length_penalty=0.6, local_rank=-1, log_dir='/root/logs', lr=0.0005, min_lr=1e-06, mixup=0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='beit3_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_beams=3, num_max_bpe_tokens=64, num_workers=10, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='', pin_mem=True, randaug=False, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=5, seed=0, sentencepiece_model='/root/code/beit3/beit3.spm', smoothing=0.1, start_epoch=0, task='sarcasm', task_cache_path='', task_head_lr_weight=0, train_interpolation='bicubic', update_freq=1, vocab_size=64010, warmup_epochs=5, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, world_size=1, zero_stage=0)
model_config = beit3_base_patch16_224_sarcasm
Model = BEiT3ForSarcasmDetection(
  (beit3): BEiT3(
    (text_embed): TextEmbedding(64010, 768)
    (vision_embed): VisionEmbedding(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (encoder): Encoder(
      (dropout_module): Dropout(p=0.0, inplace=False)
      (embed_positions): MutliwayEmbedding(
        (A): PositionalEmbedding(1223, 768)
        (B): PositionalEmbedding(1024, 768)
      )
      (layers): ModuleList(
        (0): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.0)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.009090909090909092)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.018181818181818184)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.027272727272727275)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.03636363636363637)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.04545454545454546)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.05454545454545455)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.06363636363636364)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.07272727272727274)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (9): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.08181818181818183)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (10): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.09090909090909093)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (11): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.1)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): MultiwayNetwork(
        (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (expert): Encoder(
      (dropout_module): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (inner_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.0)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          )
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pooler): Pooler(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
  (head): Sequential(
    (0): Linear(in_features=768, out_features=1536, bias=True)
    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (2): GELU(approximate=none)
    (3): Linear(in_features=1536, out_features=2, bias=True)
  )
)
number of params: 230650370
LR = 0.00050000
Batch size = 16
Update frequent = 1
Number of training examples = 19816
Number of training training per epoch = 1238
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
acc 0.855127 precision 0.860520 recall 0.759124 f1 0.806648
Start training for 20 epochs
{'epoch': 0, 'step': '0/1238', 'loss': 11.99308967590332}
{'loss_scale': 32768.0, 'lr': 0.0, 'min_lr': 0.0, 'weight_decay': 0.05, 'grad_norm': tensor(nan, device='cuda:0')}
{'epoch': 0, 'step': '10/1238', 'loss': 11.673218727111816}
{'loss_scale': 32768.0, 'lr': 8.078849571820972e-07, 'min_lr': 2.053535165882211e-07, 'weight_decay': 0.05, 'grad_norm': tensor(29.9154, device='cuda:0')}

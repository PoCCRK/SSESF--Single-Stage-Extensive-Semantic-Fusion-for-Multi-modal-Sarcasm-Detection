{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d257a71-ce82-4526-adb2-931303ed416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "img_dir = '/hy-tmp/data/dataset_image'\n",
    "train_file = '/hy-tmp/data/data-of-multimodal-sarcasm-detection/text/train.txt'\n",
    "valid_file = '/hy-tmp/data/data-of-multimodal-sarcasm-detection/text/valid2.txt'\n",
    "test_file = '/hy-tmp/data/data-of-multimodal-sarcasm-detection/text/test2.txt'\n",
    "\n",
    "image_files = os.listdir(img_dir)\n",
    "\n",
    "CM_BERT_predicts = '/root/results/CM_BERT_predicts.txt'\n",
    "CM_VIT_predicts = '/root/results/CM_VIT_predicts.txt'\n",
    "CM_BERT_TEXT_IN_IMG_TEXT_predicts = '/root/results/CM_BERT_TEXT_IN_IMG_TEXT_predicts.txt'\n",
    "# CM_GCN_predicts = '/root/results/CM_GCN_predicts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef69533f-d64e-4309-938f-c8882e820e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(data_file):\n",
    "    all_data = {}\n",
    "    with open(data_file,'r',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i]\n",
    "            data = eval(line)\n",
    "            if 'train' in test_file:\n",
    "                img_id,text,label = data\n",
    "            else:\n",
    "                img_id,text,label1,label = data\n",
    "\n",
    "            image_file = img_id+'.jpg'\n",
    "            if image_file in image_files:\n",
    "                all_data[img_id] = {'image_file': image_file, 'label':int(label)}\n",
    "    return all_data\n",
    "\n",
    "def load_predicts(predicts_file, all_data):\n",
    "    logits = {}\n",
    "    with open(predicts_file,'r',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i]\n",
    "            data = line.split()\n",
    "            img_id, predict, label, logit1, logit2 = data\n",
    "            \n",
    "            if img_id in all_data:\n",
    "                logit1 = float(re.findall('-?\\d+(?:\\.\\d+)?', logit1)[0])\n",
    "                logit2 = float(re.findall('-?\\d+(?:\\.\\d+)?', logit2)[0])\n",
    "                logits[img_id] = [logit1, logit2]\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def evaluate_acc_f1(logits_list, all_data, method='mean'):\n",
    "    num_model = len(logits_list)\n",
    "    labels = []\n",
    "    logits_tmp = [[] for i in range(num_model)]\n",
    "    \n",
    "    for img_id in all_data:\n",
    "        label = all_data[img_id]['label']\n",
    "        labels.append(label)\n",
    "        for i in range(num_model):\n",
    "            model_logits = logits_list[i]\n",
    "            logit = model_logits[img_id]\n",
    "            logits_tmp[i].append(logit)\n",
    "    for i in range(num_model):\n",
    "        logits_tmp[i] = np.array(logits_tmp[i])\n",
    "        assert len(labels) == logits_tmp[i].shape[0]\n",
    "    \n",
    "    stacked_logits = np.stack(logits_tmp,axis=0)\n",
    "    if method=='sum':\n",
    "        logits = np.sum(stacked_logits, axis=0)\n",
    "    elif method=='max':\n",
    "        logits = np.max(stacked_logits, axis=0)\n",
    "    elif method=='mean':\n",
    "        logits = np.mean(stacked_logits, axis=0)\n",
    "    else:\n",
    "        print('fusion method not find, use sum methon')\n",
    "        logits = np.sum(stacked_logits, axis=0)\n",
    "    \n",
    "    predicts = np.argmax(logits, axis=1)\n",
    "    acc = metrics.accuracy_score(labels, predicts)\n",
    "    f1 = metrics.f1_score(labels, predicts)\n",
    "    precision =  metrics.precision_score(labels, predicts)\n",
    "    recall = metrics.recall_score(labels, predicts)\n",
    "    \n",
    "    return acc, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e670d164-4c3e-471e-969e-2b006e17744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of all test data: 2409\n",
      "(0.8613532586135326, 0.8274793388429752, 0.81985670419652, 0.835245046923879)\n"
     ]
    }
   ],
   "source": [
    "all_data = load_all_data(test_file)\n",
    "\n",
    "print('len of all test data:', len(all_data))\n",
    "\n",
    "CM_BERT_logits = load_predicts(CM_BERT_predicts, all_data)\n",
    "CM_VIT_logits = load_predicts(CM_VIT_predicts, all_data)\n",
    "CM_BERT_TEXT_IN_IMG_TEXT_logits = load_predicts(CM_BERT_TEXT_IN_IMG_TEXT_predicts, all_data)\n",
    "# CM_GCN_logits = load_predicts(CM_GCN_predicts, all_data)\n",
    "\n",
    "fusion_models = [CM_BERT_logits, CM_VIT_logits, CM_BERT_TEXT_IN_IMG_TEXT_logits]\n",
    "\n",
    "result = evaluate_acc_f1(fusion_models, all_data, method='mean')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1be1bd-9c5a-4440-b22f-b4ed04d3795c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

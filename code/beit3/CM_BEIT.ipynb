{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "\n",
    "model_name = 'CM_BEIT'\n",
    "img_dir = '/hy-tmp/data/dataset_image'\n",
    "check_point_path = '/hy-tmp/models'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "log_file = f'/root/logs/{model_name}-{strftime(\"%y%m%d-%H%M\", localtime())}.log'\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.addHandler(logging.FileHandler(log_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data.transforms import RandomResizedCropAndInterpolation\n",
    "\n",
    "import utils\n",
    "from randaug import RandomAugment\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, data_path, split, transform, \n",
    "        tokenizer, num_max_bpe_tokens, task=None,\n",
    "    ):\n",
    "        index_files = self.get_index_files(split, task=task)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_max_bpe_tokens = num_max_bpe_tokens\n",
    "        self.data_path = data_path\n",
    "        items = []\n",
    "        self.index_files = index_files\n",
    "\n",
    "        offset = 0\n",
    "        for _index_file in index_files:\n",
    "            index_file = os.path.join(data_path, _index_file)\n",
    "            with open(index_file, mode=\"r\", encoding=\"utf-8\") as reader:\n",
    "                for line in reader:\n",
    "                    data = json.loads(line)\n",
    "                    items.append(data)\n",
    "                print(\"Load %d image-text pairs from %s. \" % (len(items) - offset, index_file))\n",
    "                offset = len(items)\n",
    "        self.items = items\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.loader = default_loader\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "    @staticmethod\n",
    "    def get_index_files(split):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_image(self, image_path: str):\n",
    "        image_path = os.path.join(self.data_path, image_path)\n",
    "        image = self.loader(image_path)\n",
    "        return self.transform(image)\n",
    "\n",
    "    def _get_text_segment(self, text_segment, max_len=None):\n",
    "        if isinstance(text_segment, str):\n",
    "            tokens = self.tokenizer.tokenize(text_segment)\n",
    "        else:\n",
    "            tokens = text_segment[:]\n",
    "        # if len(tokens) == 0:\n",
    "        #     raise RuntimeError(\"The text segment should contains at least one tokens!\")\n",
    "        if max_len is None:\n",
    "            max_len = self.num_max_bpe_tokens\n",
    "\n",
    "        if len(tokens) > max_len - 2:\n",
    "            tokens = tokens[:max_len - 2]\n",
    "\n",
    "        tokens = [self.bos_token_id] + tokens[:] + [self.eos_token_id]\n",
    "        num_tokens = len(tokens)\n",
    "        padding_mask = [0] * num_tokens + [1] * (max_len - num_tokens)\n",
    "        return tokens + [self.pad_token_id] * (max_len - num_tokens), padding_mask, num_tokens\n",
    "\n",
    "    def _get_image_text_example(self, index: int, data: dict):\n",
    "        item = self.items[index]\n",
    "        img_path = item[\"image_path\"]\n",
    "        img = self._get_image(img_path)\n",
    "        data[\"image\"] = img\n",
    "\n",
    "        text_segment = item[\"text_segment\"]\n",
    "        language_tokens, padding_mask, _ = self._get_text_segment(text_segment)\n",
    "        data[\"language_tokens\"] = language_tokens\n",
    "        data[\"padding_mask\"] = padding_mask\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = dict()\n",
    "        self._get_image_text_example(index, data)\n",
    "        return data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = '{' + \"\\n  Number of items: %s,\" % self.__len__()\n",
    "        body += \"\\n  data root = %s,\" % self.data_path\n",
    "        body += \"\\n  split = %s,\" % self.split\n",
    "        body += \"\\n  dataset index files = %s\" % str(self.index_files)\n",
    "        body += \"\\n  num max bpe tokens = %s\" % self.num_max_bpe_tokens\n",
    "        body += \"\\n  transforms = [\"\n",
    "        for t in self.transform.transforms:\n",
    "            body += \"\\n    %s\" % str(t)\n",
    "        body += \"\\n  ]\"\n",
    "        body += \"\\n}\"\n",
    "\n",
    "        return head + body\n",
    "\n",
    "\n",
    "def _write_data_into_jsonl(items, jsonl_file):\n",
    "    with open(jsonl_file, mode=\"w\", encoding=\"utf-8\") as writer:\n",
    "        for data in items:\n",
    "            writer.write(json.dumps(data, indent=None))\n",
    "            writer.write('\\n')\n",
    "    print(\"Write %s with %d items !\" % (jsonl_file, len(items)))\n",
    "\n",
    "\n",
    "class SarcasmDataset(BaseDataset):\n",
    "    @staticmethod\n",
    "    def get_index_files(split, task=None):\n",
    "        if split == \"train\":\n",
    "            return (\"sarcasm.train.jsonl\", )\n",
    "        elif split == \"val\":\n",
    "            return (\"sarcasm.test.jsonl\", )\n",
    "        elif split == \"test\":\n",
    "            return (\"sarcasm.test.jsonl\", )\n",
    "        else:\n",
    "            raise RuntimeError(\"split %s is not found!\" % split)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data = super().__getitem__(index)\n",
    "        data[\"label\"] = self.items[index][\"label\"]\n",
    "        data[\"image_id\"] = int(self.items[index][\"image_id\"])\n",
    "        ocr, padding_mask, _ = self._get_text_segment(self.items[index][\"ocr_segment\"])\n",
    "        data[\"ocr\"] = ocr\n",
    "        data[\"ocr_padding_mask\"] = padding_mask\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def make_dataset_index(cls, data_path, tokenizer, image_path, ocr_path=None):\n",
    "        image_names = os.listdir(image_path)\n",
    "        if ocr_path:\n",
    "            with open(ocr_path,'rb') as fin:\n",
    "                text_in_imgs = pickle.load(fin)\n",
    "        \n",
    "        items = []\n",
    "        index_file = os.path.join(data_path, f\"sarcasm.train.jsonl\")\n",
    "        with open(os.path.join(data_path, \"train.txt\"),'r',encoding='utf-8') as fin:\n",
    "            lines = fin.readlines()\n",
    "            lines = [x.strip() for x in lines]\n",
    "            for i in range(len(lines)):\n",
    "                line = lines[i]\n",
    "                data = eval(line)\n",
    "                img_id,text,label = data\n",
    "                if img_id+'.jpg' in image_names:\n",
    "                    # text = text.replace(\"#\", '')\n",
    "                    text_in_img = text_in_imgs[img_id] if ocr_path else ''\n",
    "                    if text_in_img:\n",
    "                        tokens = tokenizer.tokenize(text)\n",
    "                        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        ocr_tokens = tokenizer.tokenize(text_in_img)\n",
    "                        ocr_token_ids = tokenizer.convert_tokens_to_ids(ocr_tokens)\n",
    "                    else:\n",
    "                        tokens = tokenizer.tokenize(text)\n",
    "                        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        ocr_token_ids = []\n",
    "                    items.append({\n",
    "                            \"image_path\": os.path.join(image_path, f\"{img_id}.jpg\"),\n",
    "                            \"text_segment\": token_ids,\n",
    "                            \"ocr_segment\": ocr_token_ids,\n",
    "                            \"image_id\": img_id,\n",
    "                            \"label\": label,\n",
    "                        })\n",
    "        _write_data_into_jsonl(items, index_file)\n",
    "        items = []\n",
    "        index_file = os.path.join(data_path, f\"sarcasm.valid.jsonl\")\n",
    "        with open(os.path.join(data_path, \"valid2.txt\"),'r',encoding='utf-8') as fin:\n",
    "            lines = fin.readlines()\n",
    "            lines = [x.strip() for x in lines]\n",
    "            for i in range(len(lines)):\n",
    "                line = lines[i]\n",
    "                data = eval(line)\n",
    "                img_id,text,label1,label = data\n",
    "                if img_id+'.jpg' in image_names:\n",
    "                    # text = text.replace(\"#\", '')\n",
    "                    text_in_img = text_in_imgs[img_id] if ocr_path else ''\n",
    "                    if text_in_img:\n",
    "                        # encoded_dict = tokenizer(\n",
    "                        #                 text,                      # Sentence to encode.\n",
    "                        #                 text_in_img,\n",
    "                        #                 add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        #                 padding = 'max_length',\n",
    "                        #                 truncation = 'longest_first',\n",
    "                        #                 max_length = 64,    # Pad & truncate all sentences.\n",
    "                        #                 return_attention_mask = True,   # Construct attn. masks.\n",
    "                        #                 return_length = True,\n",
    "                        #         )\n",
    "                        # token_ids = encoded_dict.input_ids\n",
    "                        tokens = tokenizer.tokenize(text)\n",
    "                        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        ocr_tokens = tokenizer.tokenize(text_in_img)\n",
    "                        ocr_token_ids = tokenizer.convert_tokens_to_ids(ocr_tokens)\n",
    "                    else:\n",
    "                        tokens = tokenizer.tokenize(text)\n",
    "                        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        ocr_token_ids = []\n",
    "                    items.append({\n",
    "                            \"image_path\": os.path.join(image_path, f\"{img_id}.jpg\"),\n",
    "                            \"text_segment\": token_ids,\n",
    "                            \"ocr_segment\": ocr_token_ids,\n",
    "                            \"image_id\": img_id,\n",
    "                            \"label\": label,\n",
    "                        })\n",
    "        _write_data_into_jsonl(items, index_file)\n",
    "        items = []\n",
    "        index_file = os.path.join(data_path, f\"sarcasm.test.jsonl\")\n",
    "        with open(os.path.join(data_path, \"test2.txt\"),'r',encoding='utf-8') as fin:\n",
    "            lines = fin.readlines()\n",
    "            lines = [x.strip() for x in lines]\n",
    "            for i in range(len(lines)):\n",
    "                line = lines[i]\n",
    "                data = eval(line)\n",
    "                img_id,text,label1,label = data\n",
    "                if img_id+'.jpg' in image_names:\n",
    "                    # text = text.replace(\"#\", '')\n",
    "                    text_in_img = text_in_imgs[img_id] if ocr_path else ''\n",
    "                    if text_in_img:\n",
    "                        # encoded_dict = tokenizer(\n",
    "                        #                 text,                      # Sentence to encode.\n",
    "                        #                 text_in_img,\n",
    "                        #                 add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        #                 padding = 'max_length',\n",
    "                        #                 truncation = 'longest_first',\n",
    "                        #                 max_length = 64,    # Pad & truncate all sentences.\n",
    "                        #                 return_attention_mask = True,   # Construct attn. masks.\n",
    "                        #                 return_length = True,\n",
    "                        #         )\n",
    "                        # token_ids = encoded_dict.input_ids\n",
    "                        tokens = tokenizer.tokenize(text)\n",
    "                        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        ocr_tokens = tokenizer.tokenize(text_in_img)\n",
    "                        ocr_token_ids = tokenizer.convert_tokens_to_ids(ocr_tokens)\n",
    "                        token_ids = token_ids + ocr_token_ids\n",
    "                    else:\n",
    "                        tokens = tokenizer.tokenize(text)\n",
    "                        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        ocr_token_ids = []\n",
    "                    items.append({\n",
    "                            \"image_path\": os.path.join(image_path, f\"{img_id}.jpg\"),\n",
    "                            \"text_segment\": token_ids,\n",
    "                            \"ocr_segment\": ocr_token_ids,\n",
    "                            \"image_id\": img_id,\n",
    "                            \"label\": label,\n",
    "                        })\n",
    "        _write_data_into_jsonl(items, index_file)\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, is_train, batch_size, num_workers, pin_mem, dist_eval=False):\n",
    "    if is_train or dist_eval:\n",
    "        num_tasks = utils.get_world_size()\n",
    "        global_rank = utils.get_rank()\n",
    "\n",
    "        if not is_train and dist_eval and len(dataset) % num_tasks != 0:\n",
    "            print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n",
    "                    'This will slightly alter validation results as extra duplicate entries are added to achieve '\n",
    "                    'equal num of samples per-process.')\n",
    "\n",
    "        sampler = torch.utils.data.DistributedSampler(\n",
    "            dataset, num_replicas=num_tasks, rank=global_rank, shuffle=is_train\n",
    "        )\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, sampler=sampler,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem,\n",
    "        drop_last=is_train,\n",
    "        collate_fn=utils.merge_batch_tensors_by_dict_key,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    if is_train:\n",
    "        t = [\n",
    "            RandomResizedCropAndInterpolation(args.input_size, scale=(0.5, 1.0), interpolation=args.train_interpolation), \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ]\n",
    "        if args.randaug:\n",
    "            t.append(\n",
    "                RandomAugment(\n",
    "                    2, 7, isPIL=True, \n",
    "                    augs=[\n",
    "                        'Identity','AutoContrast','Equalize','Brightness','Sharpness', \n",
    "                        'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate', \n",
    "                    ]))\n",
    "        t += [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD), \n",
    "        ]\n",
    "        t = transforms.Compose(t)\n",
    "    else:\n",
    "        t = transforms.Compose([\n",
    "            transforms.Resize((args.input_size, args.input_size), interpolation=3), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)\n",
    "        ])\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def get_sentencepiece_model_for_beit3(args):\n",
    "    from transformers import XLMRobertaTokenizer\n",
    "    return XLMRobertaTokenizer(args.sentencepiece_model)\n",
    "\n",
    "\n",
    "task2dataset = {\n",
    "    # \"nlvr2\": NLVR2Dataset, \n",
    "    # \"vqav2\": VQAv2Dataset, \n",
    "    # \"flickr30k\": RetrievalDataset, \n",
    "    # \"coco_retrieval\": RetrievalDataset,  \n",
    "    # \"coco_captioning\": CaptioningDataset,\n",
    "    # \"nocaps\": CaptioningDataset,\n",
    "    # \"imagenet\": ImageNetDataset,\n",
    "    \"sarcasm\": SarcasmDataset,\n",
    "}\n",
    "\n",
    "\n",
    "def create_dataset_by_split(args, split, is_train=True):\n",
    "    transform = build_transform(is_train=is_train, args=args)\n",
    "    dataset_class = task2dataset[args.task]\n",
    "    tokenizer = get_sentencepiece_model_for_beit3(args)\n",
    "\n",
    "    opt_kwargs = {}\n",
    "    if args.task in [\"coco_captioning\", \"nocaps\"]:\n",
    "        opt_kwargs[\"mask_prob\"] = args.captioning_mask_prob\n",
    "\n",
    "    dataset = dataset_class(\n",
    "        data_path=args.data_path, split=split, \n",
    "        transform=transform, tokenizer=tokenizer, \n",
    "        num_max_bpe_tokens=args.num_max_bpe_tokens, \n",
    "        task=args.task, **opt_kwargs, \n",
    "    )\n",
    "    if is_train:\n",
    "        batch_size = args.batch_size\n",
    "    elif hasattr(args, \"eval_batch_size\") and args.eval_batch_size is not None:\n",
    "        batch_size = args.eval_batch_size\n",
    "    else:\n",
    "        batch_size = int(args.batch_size * 1.5)\n",
    "\n",
    "    return create_dataloader(\n",
    "        dataset, is_train=is_train, batch_size=batch_size, \n",
    "        num_workers=args.num_workers, pin_mem=args.pin_mem, dist_eval=args.dist_eval, \n",
    "    )\n",
    "\n",
    "\n",
    "def create_downstream_dataset(args, is_eval=False):\n",
    "    if is_eval:\n",
    "        return create_dataset_by_split(args, split=\"test\", is_train=False)\n",
    "    else:\n",
    "        return \\\n",
    "            create_dataset_by_split(args, split=\"train\", is_train=True), \\\n",
    "            create_dataset_by_split(args, split=\"val\", is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write /hy-tmp/data/data-of-multimodal-sarcasm-detection/text/sarcasm.train.jsonl with 19816 items !\n",
      "Write /hy-tmp/data/data-of-multimodal-sarcasm-detection/text/sarcasm.valid.jsonl with 2410 items !\n",
      "Write /hy-tmp/data/data-of-multimodal-sarcasm-detection/text/sarcasm.test.jsonl with 2409 items !\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(\"/root/code/beit3/beit3.spm\")\n",
    "\n",
    "SarcasmDataset.make_dataset_index(\n",
    "    data_path=\"/hy-tmp/data/data-of-multimodal-sarcasm-detection/text\",\n",
    "    tokenizer=tokenizer,\n",
    "    image_path=\"/hy-tmp/data/dataset_image\",\n",
    "    ocr_path=\"/hy-tmp/data/str_in_images_cleaned.pkl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchscale.architecture.encoder import Encoder\n",
    "from torchscale.component.embedding import (\n",
    "    PositionalEmbedding,\n",
    "    TextEmbedding,\n",
    "    VisionEmbedding,\n",
    ")\n",
    "from torchscale.component.multiway_network import MutliwayEmbedding\n",
    "\n",
    "import copy\n",
    "\n",
    "class BEiT3(nn.Module):\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        assert args.multiway\n",
    "        assert args.vocab_size > 0\n",
    "        assert not args.share_encoder_input_output_embed\n",
    "        self.text_embed = TextEmbedding(args.vocab_size, args.encoder_embed_dim)\n",
    "        self.vision_embed = VisionEmbedding(\n",
    "            args.img_size,\n",
    "            args.patch_size,\n",
    "            args.in_chans,\n",
    "            args.encoder_embed_dim,\n",
    "            contain_mask_token=True,\n",
    "            prepend_cls_token=True,\n",
    "        )\n",
    "        # being consistent with Fairseq, which starts from 2 for position embedding\n",
    "        embed_positions = MutliwayEmbedding(\n",
    "            modules=[\n",
    "                # PositionalEmbedding(self.vision_embed.num_position_embeddings() + 2 + args.max_source_positions, args.encoder_embed_dim),\n",
    "                PositionalEmbedding(self.vision_embed.num_position_embeddings() + 2, args.encoder_embed_dim),\n",
    "                PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.encoder = Encoder(\n",
    "            args,\n",
    "            embed_tokens=None,\n",
    "            embed_positions=embed_positions,\n",
    "            output_projection=None,\n",
    "            is_encoder_decoder=False,\n",
    "        )\n",
    "        # args1 = copy.copy(args)\n",
    "        # args1.multiway = False\n",
    "        # args1.encoder_layers = 1\n",
    "        # self.expert = Encoder(\n",
    "        #     args1,\n",
    "        #     embed_tokens=None,\n",
    "        #     embed_positions=None,\n",
    "        #     output_projection=None,\n",
    "        #     is_encoder_decoder=False,\n",
    "        # )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        textual_tokens=None,\n",
    "        # ocr_tokens=None,\n",
    "        visual_tokens=None,\n",
    "        text_padding_position=None,\n",
    "        # ocr_padding_position=None,\n",
    "        attn_mask=None,\n",
    "        vision_masked_position=None,\n",
    "        incremental_state=None,\n",
    "        positions=None,\n",
    "    ):\n",
    "        assert textual_tokens is not None or visual_tokens is not None\n",
    "\n",
    "        if textual_tokens is None:\n",
    "            x = self.vision_embed(visual_tokens, vision_masked_position)\n",
    "            encoder_padding_mask = None\n",
    "            multiway_split_position = -1\n",
    "        elif visual_tokens is None:\n",
    "            x = self.text_embed(textual_tokens)\n",
    "            encoder_padding_mask = text_padding_position\n",
    "            multiway_split_position = 0\n",
    "        else:\n",
    "            x1 = self.vision_embed(visual_tokens, vision_masked_position)\n",
    "            x2 = self.text_embed(textual_tokens)\n",
    "            # x3 = self.text_embed(ocr_tokens)\n",
    "            # x = torch.cat([x1, x3, x2], dim=1)\n",
    "            # multiway_split_position = x1.size(1) + x2.size(1)\n",
    "            x = torch.cat([x1, x2], dim=1)\n",
    "            multiway_split_position = x1.size(1)\n",
    "\n",
    "            if text_padding_position is not None:\n",
    "                encoder_padding_mask = torch.cat(\n",
    "                    [\n",
    "                        torch.zeros(x1.shape[:-1]).to(x1.device).bool(),\n",
    "                        # ocr_padding_position,\n",
    "                        text_padding_position,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                encoder_padding_mask = None\n",
    "\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens=None,\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            attn_mask=attn_mask,\n",
    "            token_embeddings=x,\n",
    "            multiway_split_position=multiway_split_position,\n",
    "            incremental_state=incremental_state,\n",
    "            positions=positions,\n",
    "        )\n",
    "        encoder_out[\"multiway_split_position\"] = multiway_split_position\n",
    "\n",
    "        # expert_out = self.expert(\n",
    "        #     src_tokens=None,\n",
    "        #     encoder_padding_mask=encoder_padding_mask,\n",
    "        #     attn_mask=attn_mask,\n",
    "        #     token_embeddings=encoder_out['encoder_out'],\n",
    "        #     multiway_split_position=None,\n",
    "        #     incremental_state=incremental_state,\n",
    "        #     positions=positions,\n",
    "        # )\n",
    "\n",
    "        # return encoder_out, expert_out\n",
    "        return encoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n",
    "\n",
    "from torchscale.architecture.config import EncoderConfig\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n",
    "\n",
    "\n",
    "def _get_base_config(\n",
    "        img_size=224, patch_size=16, drop_path_rate=0, \n",
    "        checkpoint_activations=None, mlp_ratio=4, vocab_size=64010, **kwargs\n",
    "):\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size, patch_size=patch_size, vocab_size=vocab_size, multiway=True, \n",
    "        layernorm_embedding=False, normalize_output=True, no_output_layer=True, \n",
    "        drop_path_rate=drop_path_rate, encoder_embed_dim=768, encoder_attention_heads=12, \n",
    "        encoder_ffn_embed_dim=int(768 * mlp_ratio), encoder_layers=12, \n",
    "        checkpoint_activations=checkpoint_activations, \n",
    "    )\n",
    "\n",
    "\n",
    "def _get_large_config(\n",
    "        img_size=224, patch_size=16, drop_path_rate=0, \n",
    "        checkpoint_activations=None, mlp_ratio=4, vocab_size=64010, **kwargs\n",
    "):\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size, patch_size=patch_size, vocab_size=vocab_size, multiway=True, \n",
    "        layernorm_embedding=False, normalize_output=True, no_output_layer=True, \n",
    "        drop_path_rate=drop_path_rate, encoder_embed_dim=1024, encoder_attention_heads=16, \n",
    "        encoder_ffn_embed_dim=int(1024 * mlp_ratio), encoder_layers=24, \n",
    "        checkpoint_activations=checkpoint_activations, \n",
    "    )\n",
    "\n",
    "\n",
    "class BEiT3Wrapper(nn.Module):\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.beit3 = BEiT3(args)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return self.beit3.encoder.num_layers\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'beit3.encoder.embed_positions.A.weight', 'beit3.vision_embed.cls_token', 'logit_scale'}\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "class TwoLayerMLP(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_features, \n",
    "            hidden_features, \n",
    "            out_features, \n",
    "            norm_layer, \n",
    "            norm_input=True, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(in_features) if norm_input else nn.Identity()\n",
    "        self.dense1 = nn.Linear(in_features, hidden_features)\n",
    "        self.norm2 = norm_layer(hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.dense2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self, input_features, output_features, norm_layer):\n",
    "        super().__init__()\n",
    "        self.norm = norm_layer(input_features)\n",
    "        self.dense = nn.Linear(input_features, output_features)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_rep = x[:, 0, :]\n",
    "        cls_rep = self.norm(cls_rep)\n",
    "        pooled_output = self.dense(cls_rep)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class attentional_pooling(nn.Module):\n",
    "    def __init__(self, embed_dim, norm_layer):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.rand(10, embed_dim))\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # self.dense = nn.Linear(embed_dim, embed_dim)\n",
    "        # self.activation = nn.Tanh()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, 2, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        bs = x.shape[0]\n",
    "        attn_output, attn_output_weights = self.multihead_attn(self.query.repeat(bs, 1, 1), x, x, key_padding_mask=key_padding_mask)\n",
    "        cls_rep = self.norm(cls_rep)\n",
    "        cls_rep = torch.mean(attn_output, 1)\n",
    "        # pooled_output = self.dense(cls_rep)\n",
    "        # pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return cls_rep\n",
    "\n",
    "\n",
    "class Sarcasm_Head(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes, norm_layer):\n",
    "        super().__init__()\n",
    "        self.pooler = attentional_pooling(embed_dim, norm_layer)\n",
    "        self.mlp = TwoLayerMLP(\n",
    "            in_features = embed_dim,\n",
    "            hidden_features = embed_dim * 2,\n",
    "            out_features = num_classes,\n",
    "            norm_layer = norm_layer,\n",
    "            norm_input=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        pooled_output = self.pooler(x, key_padding_mask)\n",
    "        return self.mlp(pooled_output)\n",
    "\n",
    "# class BEiT3ForSarcasmDetection(BEiT3Wrapper):\n",
    "#     def __init__(\n",
    "#             self, \n",
    "#             args, \n",
    "#             num_classes, \n",
    "#             norm_layer=nn.LayerNorm, \n",
    "#             **kwargs\n",
    "#     ):\n",
    "#         super(BEiT3ForSarcasmDetection, self).__init__(args=args)\n",
    "#         embed_dim = args.encoder_embed_dim\n",
    "#         self.head = Sarcasm_Head(embed_dim=embed_dim, num_classes=2, norm_layer=norm_layer)\n",
    "#         self.head.apply(self._init_weights)\n",
    "\n",
    "#     def forward(self, image, text, padding_mask, **kwargs):\n",
    "#         outputs = self.beit3(\n",
    "#             textual_tokens=text, \n",
    "#             visual_tokens=image, \n",
    "#             text_padding_position=padding_mask, \n",
    "#         )\n",
    "#         # encoder_out encoder_embedding encoder_padding_mask encoder_states l_aux multiway_split_position\n",
    "#         x = outputs[\"encoder_out\"]\n",
    "#         return self.head(x, outputs[\"encoder_padding_mask\"].bool()), outputs\n",
    "\n",
    "class BEiT3ForSarcasmDetection(BEiT3Wrapper):\n",
    "    def __init__(\n",
    "            self, \n",
    "            args, \n",
    "            num_classes, \n",
    "            norm_layer=nn.LayerNorm, \n",
    "            **kwargs\n",
    "    ):\n",
    "        super(BEiT3ForSarcasmDetection, self).__init__(args=args)\n",
    "        embed_dim = args.encoder_embed_dim\n",
    "        self.pooler = Pooler(\n",
    "            input_features=embed_dim, \n",
    "            output_features=embed_dim, \n",
    "            norm_layer=norm_layer, \n",
    "        )\n",
    "        self.pooler.apply(self._init_weights)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2), \n",
    "            norm_layer(embed_dim * 2), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(embed_dim * 2, num_classes), \n",
    "        )\n",
    "        self.head.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, image, text, padding_mask, ocr, ocr_padding_mask, **kwargs):\n",
    "        # encoder_out encoder_embedding encoder_padding_mask encoder_states l_aux multiway_split_position\n",
    "        encoder_out = self.beit3(\n",
    "            textual_tokens=text, \n",
    "            visual_tokens=image, \n",
    "            text_padding_position=padding_mask, \n",
    "            # ocr_tokens=ocr,\n",
    "            # ocr_padding_position=ocr_padding_mask,\n",
    "        )\n",
    "        out = encoder_out[\"encoder_out\"]\n",
    "        cls = self.pooler(out)\n",
    "        logits = self.head(cls)\n",
    "\n",
    "        return logits, encoder_out\n",
    "\n",
    "\n",
    "from timm.models.registry import register_model\n",
    "@register_model\n",
    "def beit3_base_patch16_224_sarcasm(pretrained=False, **kwargs):\n",
    "    args = _get_base_config(**kwargs)\n",
    "    model = BEiT3ForSarcasmDetection(args, num_classes=2, **kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Optional\n",
    "from sklearn import metrics\n",
    "from timm.utils import ModelEma\n",
    "\n",
    "\n",
    "class TaskHandler(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.split = None\n",
    "\n",
    "    def train_batch(self, model, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def eval_batch(self, model, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def before_eval(self, data_loader, **kwargs):\n",
    "        self.split = data_loader.dataset.split\n",
    "\n",
    "    def after_eval(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class SarcasmHandler(TaskHandler):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.predictions = []\n",
    "        self.tokenizer = get_sentencepiece_model_for_beit3(args)\n",
    "        # self.num_beams = args.num_beams\n",
    "        self.max_len = args.num_max_bpe_tokens\n",
    "        # self.length_penalty = args.length_penalty\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.supconloss = utils.SupConLoss(temperature=0.07, contrast_mode='all', base_temperature=0.07)\n",
    "\n",
    "    def train_batch(self, model, image, language_tokens, padding_mask, label, ocr, ocr_padding_mask, **kwargs):\n",
    "        bsz = label.shape[0]\n",
    "        logits, encoder_out = model(\n",
    "            image=image, text=language_tokens, \n",
    "            padding_mask=padding_mask,\n",
    "            ocr=ocr, ocr_padding_mask=ocr_padding_mask)\n",
    "        celoss = self.criterion(logits, label)\n",
    "\n",
    "        # print(outputs['encoder_embedding'].shape)\n",
    "        # print(outputs['encoder_padding_mask'].shape)\n",
    "        # print(outputs, label)\n",
    "\n",
    "        feature = encoder_out['encoder_out']\n",
    "        multiway_split_position = encoder_out[\"multiway_split_position\"]\n",
    "\n",
    "        vision_cls = feature[:, 0, :]\n",
    "        language_cls = feature[:, multiway_split_position, :]\n",
    "        \n",
    "        feature = encoder_out['encoder_out'] * (1 - encoder_out['encoder_padding_mask'].unsqueeze(-1).type_as(encoder_out['encoder_embedding']))\n",
    "        # feature = outputs['encoder_out']\n",
    "\n",
    "        # embedding_dim = feature.shape[-1]\n",
    "        # feature = F.layer_norm(feature, (embedding_dim,))\n",
    "        feature = F.normalize(feature, dim=-1)\n",
    "        scloss = self.supconloss(feature, label)\n",
    "        # print(celoss, scloss)\n",
    "        cost = celoss + scloss\n",
    "        return {\n",
    "            \"loss\": cost,\n",
    "        }\n",
    "    \n",
    "    def before_eval(self, metric_logger, data_loader, **kwargs):\n",
    "        self.predictions.clear()\n",
    "\n",
    "    def eval_batch(self, model, image, language_tokens, padding_mask, label, image_id, ocr, ocr_padding_mask):\n",
    "        logits, encoder_out = model(\n",
    "            image=image, text=language_tokens, \n",
    "            padding_mask=padding_mask,\n",
    "            ocr=ocr, ocr_padding_mask=ocr_padding_mask)\n",
    "        bs = language_tokens.shape[0]\n",
    "\n",
    "        self.predictions.append({\n",
    "                \"logits\": logits.cpu(),\n",
    "                \"label\": label.cpu(),\n",
    "                \"image_id\": image_id.cpu(),\n",
    "            })\n",
    "    \n",
    "    def after_eval(self, **kwargs):\n",
    "        logits_all, labels_all, img_id_all = None, None, None\n",
    "        for b in self.predictions:\n",
    "            if logits_all is None:\n",
    "                logits_all = b['logits']\n",
    "                labels_all = b['label']\n",
    "                img_id_all = b['image_id']\n",
    "            else:\n",
    "                logits_all = torch.cat((logits_all, b['logits']), dim=0)\n",
    "                labels_all = torch.cat((labels_all, b['label']), dim=0)\n",
    "                img_id_all = torch.cat((img_id_all, b['image_id']), dim=0)\n",
    "        \n",
    "        f1 = metrics.f1_score(labels_all, torch.argmax(logits_all, -1))\n",
    "        precision = metrics.precision_score(labels_all, torch.argmax(logits_all, -1))\n",
    "        recall = metrics.recall_score(labels_all, torch.argmax(logits_all, -1))\n",
    "        acc = metrics.accuracy_score(labels_all, torch.argmax(logits_all, -1))\n",
    "        logger.info(f\"acc {acc:4f} precision {precision:4f} recall {recall:4f} f1 {f1:4f}\")\n",
    "\n",
    "        preds_all = torch.argmax(logits_all, -1).tolist()\n",
    "        logits_all = logits_all.tolist()\n",
    "        labels_all = labels_all.tolist()\n",
    "        img_id_all = img_id_all.tolist()\n",
    "        result = []\n",
    "        for logit, pred, label, image_id in zip(logits_all, preds_all, labels_all, img_id_all):\n",
    "            result.append({'logit':logit, \"prediction\":pred, 'label':label, \"image_id\":image_id, })\n",
    "        self.predictions = result\n",
    "        return result\n",
    "    \n",
    "\n",
    "def get_handler(args):\n",
    "    # if args.task == \"nlvr2\":\n",
    "    #     return NLVR2Handler()\n",
    "    # elif args.task == \"vqav2\":\n",
    "    #     return VQAHandler()\n",
    "    # elif args.task in (\"flickr30k\", \"coco_retrieval\"):\n",
    "    #     return RetrievalHandler()\n",
    "    # elif args.task in (\"coco_captioning\", \"nocaps\"):\n",
    "    #     return CaptioningHandler(args)\n",
    "    # elif args.task in (\"imagenet\"):\n",
    "    #     return ImageNetHandler(args)\n",
    "    # elif args.task in (\"sarcasm\"):\n",
    "    return SarcasmHandler(args)\n",
    "    # else:\n",
    "    #     raise NotImplementedError(\"Sorry, %s is not support.\" % args.task)\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        model: torch.nn.Module, data_loader: Iterable, \n",
    "        optimizer: torch.optim.Optimizer, device: torch.device, \n",
    "        handler: TaskHandler, epoch: int, start_steps: int, \n",
    "        lr_schedule_values: list, loss_scaler, max_norm: float = 0, \n",
    "        update_freq: int = 1, model_ema: Optional[ModelEma] = None, \n",
    "        log_writer: Optional[logging.Logger] = None, \n",
    "        task = None, mixup_fn=None,\n",
    "):\n",
    "    model.train(True)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 10\n",
    "\n",
    "    if loss_scaler is None:\n",
    "        model.zero_grad()\n",
    "        model.micro_steps = 0\n",
    "    else:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for data_iter_step, data in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        step = data_iter_step // update_freq\n",
    "        global_step = start_steps + step  # global training iteration\n",
    "        # Update LR & WD for the first acc\n",
    "        if lr_schedule_values is not None and data_iter_step % update_freq == 0:\n",
    "            for i, param_group in enumerate(optimizer.param_groups):\n",
    "                if lr_schedule_values is not None:\n",
    "                    param_group[\"lr\"] = lr_schedule_values[global_step] * param_group[\"lr_scale\"]\n",
    "        # put input data into cuda\n",
    "        for tensor_key in data.keys():\n",
    "            data[tensor_key] = data[tensor_key].to(device, non_blocking=True)\n",
    "            # print(\"input %s = %s\" % (tensor_key, data[tensor_key]))\n",
    "            if loss_scaler is None and tensor_key.startswith(\"image\"):\n",
    "                data[tensor_key] = data[tensor_key].half()\n",
    "\n",
    "        # mixup for imagenet finetuning\n",
    "        if mixup_fn is not None:\n",
    "            data[\"image\"], data[\"label\"] = mixup_fn(data[\"image\"], data[\"label\"])\n",
    "        \n",
    "        if task in [\"coco_captioning\", \"nocaps\"]:\n",
    "            data[\"global_step\"] = global_step\n",
    "\n",
    "        if loss_scaler is None:\n",
    "            results = handler.train_batch(model, **data)\n",
    "        else:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                results = handler.train_batch(model, **data)\n",
    "\n",
    "        loss = results.pop(\"loss\")\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        if loss_scaler is None:\n",
    "            loss /= update_freq\n",
    "            model.backward(loss)\n",
    "            model.step()\n",
    "\n",
    "            if (data_iter_step + 1) % update_freq == 0:\n",
    "                # model.zero_grad()\n",
    "                # Deepspeed will call step() & model.zero_grad() automatic\n",
    "                if model_ema is not None:\n",
    "                    model_ema.update(model)\n",
    "            grad_norm = None\n",
    "            loss_scale_value = utils.get_loss_scale_for_deepspeed(model)\n",
    "        else:\n",
    "            # this attribute is added by timm on one optimizer (adahessian)\n",
    "            is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "            loss /= update_freq\n",
    "            grad_norm = loss_scaler(loss, optimizer, clip_grad=max_norm,\n",
    "                                    parameters=model.parameters(), create_graph=is_second_order,\n",
    "                                    update_grad=(data_iter_step + 1) % update_freq == 0)\n",
    "            if (data_iter_step + 1) % update_freq == 0:\n",
    "                optimizer.zero_grad()\n",
    "                if model_ema is not None:\n",
    "                    model_ema.update(model)\n",
    "            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(loss_scale=loss_scale_value)\n",
    "        min_lr = 10.\n",
    "        max_lr = 0.\n",
    "        for group in optimizer.param_groups:\n",
    "            min_lr = min(min_lr, group[\"lr\"])\n",
    "            max_lr = max(max_lr, group[\"lr\"])\n",
    "\n",
    "        metric_logger.update(lr=max_lr)\n",
    "        metric_logger.update(min_lr=min_lr)\n",
    "        weight_decay_value = None\n",
    "        for group in optimizer.param_groups:\n",
    "            if group[\"weight_decay\"] > 0:\n",
    "                weight_decay_value = group[\"weight_decay\"]\n",
    "        metric_logger.update(weight_decay=weight_decay_value)\n",
    "        metric_logger.update(grad_norm=grad_norm)\n",
    "\n",
    "        if log_writer is not None:\n",
    "            kwargs = {\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": f\"{step}/{len(data_loader)}\",\n",
    "                \"loss\": loss_value, \n",
    "            }\n",
    "            for key in results:\n",
    "                kwargs[key] = results[key]\n",
    "            log_writer.debug(kwargs)\n",
    "            if global_step % print_freq == 0 or global_step == len(data_loader) - 1:\n",
    "                log_writer.info(kwargs)\n",
    "\n",
    "            kwargs = {\n",
    "                \"loss_scale\": loss_scale_value, \n",
    "                \"lr\": max_lr, \n",
    "                \"min_lr\": min_lr, \n",
    "                \"weight_decay\": weight_decay_value, \n",
    "                \"grad_norm\": grad_norm, \n",
    "            }\n",
    "            log_writer.debug(kwargs)\n",
    "            if global_step % print_freq == 0 or global_step == len(data_loader) - 1:\n",
    "                log_writer.info(kwargs)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(data_loader, model, device, handler, log_writer: Optional[logging.Logger] = None):\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    handler.before_eval(metric_logger=metric_logger, data_loader=data_loader)\n",
    "\n",
    "    for data in metric_logger.log_every(data_loader, 10, header):\n",
    "        for tensor_key in data.keys():\n",
    "            data[tensor_key] = data[tensor_key].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            handler.eval_batch(model=model, **data)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    results = handler.after_eval()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        kwargs = {\n",
    "            \"header\": header,\n",
    "        }\n",
    "        for key in results:\n",
    "            kwargs[key] = results[key]\n",
    "        log_writer.info(kwargs)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Load 19816 image-text pairs from /hy-tmp/data/data-of-multimodal-sarcasm-detection/text/sarcasm.train.jsonl. \n",
      "Load 2409 image-text pairs from /hy-tmp/data/data-of-multimodal-sarcasm-detection/text/sarcasm.test.jsonl. \n",
      "Load ckpt from /hy-tmp/models/best_state/checkpoint-best.pth\n",
      "Load state_dict by model_key = model\n",
      "Weights of BEiT3ForSarcasmDetection not initialized from pretrained model: ['beit3.encoder.layer_norm.A.weight', 'beit3.encoder.layer_norm.A.bias', 'beit3.encoder.layer_norm.B.weight', 'beit3.encoder.layer_norm.B.bias']\n",
      "Param groups = {\n",
      "  \"layer_0_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.text_embed.weight\",\n",
      "      \"beit3.vision_embed.mask_token\",\n",
      "      \"beit3.vision_embed.proj.weight\",\n",
      "      \"beit3.encoder.embed_positions.B.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001\n",
      "  },\n",
      "  \"layer_0_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.vision_embed.cls_token\",\n",
      "      \"beit3.vision_embed.proj.bias\",\n",
      "      \"beit3.encoder.embed_positions.A.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2541865828329001\n",
      "  },\n",
      "  \"layer_1_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001\n",
      "  },\n",
      "  \"layer_1_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.0.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.0.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.0.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.2824295364810001\n",
      "  },\n",
      "  \"layer_2_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006\n",
      "  },\n",
      "  \"layer_2_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.1.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.1.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.1.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.31381059609000006\n",
      "  },\n",
      "  \"layer_3_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001\n",
      "  },\n",
      "  \"layer_3_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.2.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.2.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.2.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3486784401000001\n",
      "  },\n",
      "  \"layer_4_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001\n",
      "  },\n",
      "  \"layer_4_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.3.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.3.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.3.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.3874204890000001\n",
      "  },\n",
      "  \"layer_5_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001\n",
      "  },\n",
      "  \"layer_5_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.4.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.4.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.4.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4304672100000001\n",
      "  },\n",
      "  \"layer_6_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001\n",
      "  },\n",
      "  \"layer_6_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.5.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.5.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.5.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.4782969000000001\n",
      "  },\n",
      "  \"layer_7_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441\n",
      "  },\n",
      "  \"layer_7_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.6.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.6.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.6.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.531441\n",
      "  },\n",
      "  \"layer_8_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001\n",
      "  },\n",
      "  \"layer_8_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.7.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.7.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.7.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.5904900000000001\n",
      "  },\n",
      "  \"layer_9_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561\n",
      "  },\n",
      "  \"layer_9_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.8.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.8.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.8.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.6561\n",
      "  },\n",
      "  \"layer_10_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001\n",
      "  },\n",
      "  \"layer_10_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.9.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.9.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.9.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.7290000000000001\n",
      "  },\n",
      "  \"layer_11_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81\n",
      "  },\n",
      "  \"layer_11_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.10.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.10.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.10.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.81\n",
      "  },\n",
      "  \"layer_12_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.B.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc1.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc2.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc1.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9\n",
      "  },\n",
      "  \"layer_12_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.k_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.v_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.q_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.out_proj.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn.inner_attn_ln.B.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.11.self_attn_layer_norm.B.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc1.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.fc2.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.A.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc1.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.fc2.bias\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.ffn_layernorm.weight\",\n",
      "      \"beit3.encoder.layers.11.ffn.B.ffn_layernorm.bias\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layers.11.final_layer_norm.B.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 0.9\n",
      "  },\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"params\": [\n",
      "      \"beit3.encoder.layer_norm.A.weight\",\n",
      "      \"beit3.encoder.layer_norm.A.bias\",\n",
      "      \"beit3.encoder.layer_norm.B.weight\",\n",
      "      \"beit3.encoder.layer_norm.B.bias\",\n",
      "      \"pooler.norm.weight\",\n",
      "      \"pooler.norm.bias\",\n",
      "      \"pooler.dense.bias\",\n",
      "      \"head.0.bias\",\n",
      "      \"head.1.weight\",\n",
      "      \"head.1.bias\",\n",
      "      \"head.3.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  },\n",
      "  \"layer_13_decay\": {\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"params\": [\n",
      "      \"pooler.dense.weight\",\n",
      "      \"head.0.weight\",\n",
      "      \"head.3.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0\n",
      "  }\n",
      "}\n",
      "Set warmup steps = 6190\n",
      "Auto resume checkpoint: \n",
      "Load 2409 image-text pairs from /hy-tmp/data/data-of-multimodal-sarcasm-detection/text/sarcasm.test.jsonl. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/101]  eta: 0:08:09    time: 4.8499  data: 1.3596  max mem: 1506\n",
      "Test:  [ 10/101]  eta: 0:00:57    time: 0.6353  data: 0.1238  max mem: 1506\n",
      "Test:  [ 20/101]  eta: 0:00:35    time: 0.2140  data: 0.0002  max mem: 1506\n",
      "Test:  [ 30/101]  eta: 0:00:25    time: 0.2143  data: 0.0002  max mem: 1506\n",
      "Test:  [ 40/101]  eta: 0:00:19    time: 0.2145  data: 0.0002  max mem: 1506\n",
      "Test:  [ 50/101]  eta: 0:00:15    time: 0.2150  data: 0.0002  max mem: 1506\n",
      "Test:  [ 60/101]  eta: 0:00:11    time: 0.2160  data: 0.0002  max mem: 1506\n",
      "Test:  [ 70/101]  eta: 0:00:08    time: 0.2176  data: 0.0003  max mem: 1506\n",
      "Test:  [ 80/101]  eta: 0:00:05    time: 0.2161  data: 0.0003  max mem: 1506\n",
      "Test:  [ 90/101]  eta: 0:00:02    time: 0.2108  data: 0.0002  max mem: 1506\n",
      "Test:  [100/101]  eta: 0:00:00    time: 0.2044  data: 0.0001  max mem: 1506\n",
      "Test: Total time: 0:00:26 (0.2598 s / it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/1238]  eta: 0:57:22  lr: 0.000000  min_lr: 0.000000  loss: 12.1132 (12.1132)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.8409 (20.8409)  time: 2.7808  data: 1.3694  max mem: 3453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 514>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opts\u001b[38;5;241m.\u001b[39moutput_dir:\n\u001b[1;32m    517\u001b[0m     Path(opts\u001b[38;5;241m.\u001b[39moutput_dir)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 518\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_init\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, ds_init)\u001b[0m\n\u001b[1;32m    423\u001b[0m     data_loader_train\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# if log_writer is not None:\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m#     log_writer.set_step(epoch * num_training_steps_per_epoch * args.update_freq)\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_training_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedule_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_writer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39msave_ckpt:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39msave_ckpt_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m args\u001b[38;5;241m.\u001b[39mepochs:\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device, handler, epoch, start_steps, lr_schedule_values, loss_scaler, max_norm, update_freq, model_ema, log_writer, task, mixup_fn)\u001b[0m\n\u001b[1;32m    198\u001b[0m is_second_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_second_order\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mis_second_order\n\u001b[1;32m    199\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m update_freq\n\u001b[0;32m--> 200\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mloss_scaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_second_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mupdate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mupdate_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data_iter_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/code/beit3/utils.py:391\u001b[0m, in \u001b[0;36mNativeScalerWithGradNormCount.__call__\u001b[0;34m(self, loss, optimizer, clip_grad, parameters, create_graph, update_grad)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    390\u001b[0m         norm \u001b[38;5;241m=\u001b[39m get_grad_norm_(parameters)\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:285\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 285\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    262\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    268\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks (https://arxiv.org/abs/2208.10442)\n",
    "# Github source: https://github.com/microsoft/unilm/tree/master/beit3\n",
    "# Copyright (c) 2023 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# --------------------------------------------------------'\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.models import create_model\n",
    "from timm.utils import ModelEma\n",
    "from optim_factory import create_optimizer, get_parameter_groups, \\\n",
    "    LayerDecayValueAssigner, get_is_head_flag_for_vit\n",
    "\n",
    "# from engine_for_finetuning import train_one_epoch, get_handler, evaluate\n",
    "# from datasets import create_downstream_dataset\n",
    "from utils import NativeScalerWithGradNormCount as NativeScaler\n",
    "import utils\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser('BEiT fine-tuning and evaluation script for image classification', add_help=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='beit_base_patch16_224', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--task', type=str, required=True, \n",
    "                        choices=['nlvr2', 'vqav2', 'flickr30k', 'coco_retrieval', 'coco_captioning', 'nocaps', 'imagenet', 'sarcasm'], \n",
    "                        help='Name of task to fine-tuning')\n",
    "\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "    parser.add_argument('--drop_path', type=float, default=0.1, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.1)')\n",
    "\n",
    "    parser.add_argument('--checkpoint_activations', action='store_true', default=True, \n",
    "                        help='Enable checkpointing to save your memory.')\n",
    "    parser.add_argument('--sentencepiece_model', type=str, required=True, \n",
    "                        help='Sentencepiece model path for the pretrained model.')\n",
    "    parser.add_argument('--vocab_size', type=int, default=64010)\n",
    "    parser.add_argument('--num_max_bpe_tokens', type=int, default=64)\n",
    "\n",
    "    parser.add_argument('--model_ema', action='store_true', default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', action='store_true', default=False, help='')\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=[0.9, 0.999], type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: 0.9, 0.999, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=5e-4, metavar='LR',\n",
    "                        help='learning rate (default: 5e-4)')\n",
    "    parser.add_argument('--layer_decay', type=float, default=0.9)\n",
    "    parser.add_argument('--task_head_lr_weight', type=float, default=0)\n",
    "\n",
    "    parser.add_argument('--warmup_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='warmup learning rate (default: 1e-6)')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    parser.add_argument('--batch_size', default=64, type=int)\n",
    "    parser.add_argument('--eval_batch_size', default=None, type=int)\n",
    "    parser.add_argument('--epochs', default=20, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_freq', default=5, type=int)\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--randaug', action='store_true', default=False)\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Finetuning params\n",
    "    parser.add_argument('--finetune', default='',\n",
    "                        help='finetune from checkpoint')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str)\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "                        help='dataset path')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default=None,\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', action='store_true')\n",
    "    parser.add_argument('--no_auto_resume', action='store_false', dest='auto_resume')\n",
    "    parser.set_defaults(auto_resume=True)\n",
    "\n",
    "    parser.add_argument('--save_ckpt', action='store_true')\n",
    "    parser.add_argument('--no_save_ckpt', action='store_false', dest='save_ckpt')\n",
    "    parser.set_defaults(save_ckpt=True)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    # parameter for dump predictions (VQA, COCO captioning, NoCaps)\n",
    "    parser.add_argument('--task_cache_path', default=None, type=str)\n",
    "\n",
    "    # parameter for imagenet finetuning\n",
    "    parser.add_argument('--nb_classes', default=1000, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--mixup', type=float, default=0,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "    \n",
    "    # augmentation parameters for imagenet finetuning\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)')\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "\n",
    "    # evaluation parameters for imagenet\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "\n",
    "    # random Erase params for imagenet finetuning\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # parameter for captioning finetuning\n",
    "    parser.add_argument('--captioning_mask_prob', type=float, default=0.6)\n",
    "    parser.add_argument('--drop_worst_ratio', type=float, default=0.2)\n",
    "    parser.add_argument('--drop_worst_after', type=int, default=12000)\n",
    "    parser.add_argument('--num_beams', type=int, default=3)\n",
    "    parser.add_argument('--length_penalty', type=float, default=0.6)\n",
    "\n",
    "    # label smoothing for imagenet and captioning\n",
    "    parser.add_argument('--label_smoothing', type=float, default=0.1)\n",
    "\n",
    "    # deepspeed parameters\n",
    "    parser.add_argument('--enable_deepspeed', action='store_true', default=False)\n",
    "    parser.add_argument('--initial_scale_power', type=int, default=16)\n",
    "    parser.add_argument('--zero_stage', default=0, type=int,\n",
    "                        help='ZeRO optimizer stage (default: 0)')\n",
    "\n",
    "    # args = [\n",
    "    # '--model', 'beit3_base_patch16_224',\n",
    "    # '--input_size', '224',\n",
    "    # '--task', 'sarcasm',\n",
    "    # '--batch_size', '16',\n",
    "    # '--layer_decay', '1.0',\n",
    "    # '--lr', '3e-5',\n",
    "    # '--update_freq', '1',\n",
    "    # # '--randaug',\n",
    "    # '--epochs', '10',\n",
    "    # '--warmup_epochs', '1',\n",
    "    # '--drop_path', '0.1',\n",
    "    # '--sentencepiece_model', '/root/code/beit3/beit3.spm',\n",
    "    # '--finetune', '/hy-tmp/models/beit3_base/beit3_base_patch16_224.pth',\n",
    "    # '--data_path', '/hy-tmp/data/data-of-multimodal-sarcasm-detection/text',\n",
    "    # '--output_dir', '/hy-tmp/models/best_state',\n",
    "    # '--log_dir', '/root/logs',\n",
    "    # '--weight_decay', '0.01',\n",
    "    # '--seed', '42',\n",
    "    # '--save_ckpt_freq', '5',\n",
    "    # '--task_head_lr_weight', '20.0',\n",
    "    # '--opt_betas', '0.9', '0.98',\n",
    "    # ]\n",
    "    args = [\n",
    "    '--model', 'beit3_base_patch16_224',\n",
    "    '--input_size', '224',\n",
    "    '--task', 'sarcasm',\n",
    "    '--batch_size', '16',\n",
    "    '--sentencepiece_model', '/root/code/beit3/beit3.spm',\n",
    "    '--finetune', '/hy-tmp/models/best_state/checkpoint-best.pth',\n",
    "    '--data_path', '/hy-tmp/data/data-of-multimodal-sarcasm-detection/text',\n",
    "    '--log_dir', '/root/logs',\n",
    "    '--eval',\n",
    "    ]\n",
    "\n",
    "    known_args, _ = parser.parse_known_args(args)\n",
    "    if known_args.enable_deepspeed:\n",
    "        try:\n",
    "            import deepspeed\n",
    "            from deepspeed import DeepSpeedConfig\n",
    "            parser = deepspeed.add_config_arguments(parser)\n",
    "            ds_init = deepspeed.initialize\n",
    "        except:\n",
    "            print(\"Please 'pip install deepspeed==0.4.0'\")\n",
    "            exit(0)\n",
    "    else:\n",
    "        ds_init = None\n",
    "\n",
    "    return parser.parse_args(args), ds_init\n",
    "\n",
    "\n",
    "def main(args, ds_init):\n",
    "    utils.init_distributed_mode(args)\n",
    "\n",
    "    if ds_init is not None:\n",
    "        utils.create_ds_config(args)\n",
    "\n",
    "    if args.task_cache_path is None:\n",
    "        args.task_cache_path = args.output_dir\n",
    "\n",
    "    logger.info(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # random.seed(seed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    if utils.get_rank() == 0 and args.log_dir is not None:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        # log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n",
    "        log_writer = logger\n",
    "    else:\n",
    "        log_writer = None\n",
    "\n",
    "    data_loader_train, data_loader_val = create_downstream_dataset(args)\n",
    "\n",
    "    if not args.model.endswith(args.task):\n",
    "        if args.task in (\"flickr30k\", \"coco_retrieval\"):\n",
    "            model_config = \"%s_retrieval\" % args.model\n",
    "        elif args.task in (\"coco_captioning\", \"nocaps\"):\n",
    "            model_config = \"%s_captioning\" % args.model\n",
    "        elif args.task in (\"imagenet\"):\n",
    "            model_config = \"%s_imageclassification\" % args.model\n",
    "        else:\n",
    "            model_config = \"%s_%s\" % (args.model, args.task)\n",
    "    else:\n",
    "        model_config = args.model\n",
    "    logger.info(\"model_config = %s\" % model_config)\n",
    "    model = create_model(\n",
    "        model_config,\n",
    "        pretrained=False,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        vocab_size=args.vocab_size,\n",
    "        checkpoint_activations=args.checkpoint_activations,\n",
    "    )\n",
    "\n",
    "    if args.finetune:\n",
    "        utils.load_model_and_may_interpolate(args.finetune, model, args.model_key, args.model_prefix)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n",
    "        model_ema = ModelEma(\n",
    "            model,\n",
    "            decay=args.model_ema_decay,\n",
    "            device='cpu' if args.model_ema_force_cpu else '',\n",
    "            resume='')\n",
    "        logger.info(\"Using EMA with decay = %.8f\" % args.model_ema_decay)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    logger.info(\"Model = %s\" % str(model_without_ddp))\n",
    "    logger.info('number of params: %d' %  n_parameters)\n",
    "\n",
    "    total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n",
    "    num_training_steps_per_epoch = len(data_loader_train.dataset) // total_batch_size\n",
    "    logger.info(\"LR = %.8f\" % args.lr)\n",
    "    logger.info(\"Batch size = %d\" % total_batch_size)\n",
    "    logger.info(\"Update frequent = %d\" % args.update_freq)\n",
    "    logger.info(\"Number of training examples = %d\" % len(data_loader_train.dataset))\n",
    "    logger.info(\"Number of training training per epoch = %d\" % num_training_steps_per_epoch)\n",
    "\n",
    "    num_layers = model_without_ddp.get_num_layers()\n",
    "    if args.layer_decay < 1.0:\n",
    "        lrs = list(args.layer_decay ** (num_layers + 1 - i) for i in range(num_layers + 2))\n",
    "        assigner = LayerDecayValueAssigner(lrs)\n",
    "    elif args.task_head_lr_weight > 1:\n",
    "        assigner = LayerDecayValueAssigner([1.0, args.task_head_lr_weight], scale_handler=get_is_head_flag_for_vit)\n",
    "    else:\n",
    "        assigner = None\n",
    "\n",
    "    if assigner is not None:\n",
    "        logger.info(\"Assigned values = %s\" % str(assigner.values))\n",
    "\n",
    "    skip_weight_decay_list = model.no_weight_decay()\n",
    "\n",
    "    if args.distributed:\n",
    "        torch.distributed.barrier()\n",
    "    if args.enable_deepspeed:\n",
    "        loss_scaler = None\n",
    "        optimizer_params = get_parameter_groups(\n",
    "            model, args.weight_decay, skip_weight_decay_list,\n",
    "            assigner.get_layer_id if assigner is not None else None,\n",
    "            assigner.get_scale if assigner is not None else None)\n",
    "        model, optimizer, _, _ = ds_init(\n",
    "            args=args, model=model, model_parameters=optimizer_params,\n",
    "            dist_init_required=not args.distributed,\n",
    "        )\n",
    "\n",
    "        logger.info(\"model.gradient_accumulation_steps() = %d\" % model.gradient_accumulation_steps())\n",
    "        assert model.gradient_accumulation_steps() == args.update_freq\n",
    "    else:\n",
    "        if args.distributed:\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n",
    "            model_without_ddp = model.module\n",
    "\n",
    "        optimizer = create_optimizer(\n",
    "            args, model_without_ddp, skip_list=skip_weight_decay_list,\n",
    "            get_num_layer=assigner.get_layer_id if assigner is not None else None, \n",
    "            get_layer_scale=assigner.get_scale if assigner is not None else None)\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "    lr_schedule_values = utils.cosine_scheduler(\n",
    "        args.lr, args.min_lr, args.epochs, num_training_steps_per_epoch,\n",
    "        warmup_epochs=args.warmup_epochs, warmup_steps=args.warmup_steps,\n",
    "    )\n",
    "\n",
    "    utils.auto_load_model(\n",
    "        args=args, model=model, model_without_ddp=model_without_ddp,\n",
    "        optimizer=optimizer, loss_scaler=loss_scaler, model_ema=model_ema)\n",
    "\n",
    "    task_handler = get_handler(args)\n",
    "\n",
    "    # mixup for imagenet\n",
    "    mixup_fn = None\n",
    "    if args.task in [\"imagenet\", \"in1k\"]:\n",
    "        mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "        if mixup_active:\n",
    "            logger.info(\"Mixup is activated!\")\n",
    "            mixup_fn = Mixup(\n",
    "                mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n",
    "                prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n",
    "                label_smoothing=args.label_smoothing, num_classes=args.nb_classes)\n",
    "\n",
    "    if args.eval:\n",
    "        data_loader_test = create_downstream_dataset(args, is_eval=True)\n",
    "        if args.task in [\"nlvr2\", \"flickr30k\", \"coco_retrieval\", \"imagenet\"]:\n",
    "            ext_test_stats, task_key = evaluate(data_loader_test, model, device, task_handler)\n",
    "            logger.info(f\"Accuracy of the network on the {len(data_loader_test.dataset)} test images: {ext_test_stats[task_key]:.3f}%\")\n",
    "            exit(0)\n",
    "        elif args.task == \"vqav2\":\n",
    "            result, _ = evaluate(data_loader_test, model, device, task_handler)\n",
    "            utils.dump_predictions(args, result, \"vqav2_test\")\n",
    "            exit(0)\n",
    "        elif args.task in [\"coco_captioning\", \"nocaps\"]:\n",
    "            predictions, _ = evaluate(data_loader_test, model, device, task_handler)\n",
    "            prediction_file = utils.dump_predictions(args, predictions, \"{}_test\".format(args.task))\n",
    "            if utils.is_main_process() and args.task == \"coco_captioning\":\n",
    "                captioning_result = utils.coco_caption_eval(args.output_dir, prediction_file, \"{}_test\".format(args.task))\n",
    "                result_file = os.path.join(args.output_dir, f\"{args.task}_result.json\")\n",
    "                logger.info(json.dumps(captioning_result))\n",
    "                utils.write_result_to_jsonl(captioning_result, result_file)\n",
    "            exit(0)\n",
    "        elif args.task == \"sarcasm\":\n",
    "            predictions = evaluate(data_loader_test, model, device, task_handler)\n",
    "            result_file = os.path.join(args.output_dir, f\"{args.task}_test.json\")\n",
    "            with open(result_file, \"w\") as fp:\n",
    "                json.dump(predictions, fp, indent=2)\n",
    "            exit(0)\n",
    "\n",
    "    logger.info(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    max_accuracy = 0.0\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            data_loader_train.sampler.set_epoch(epoch)\n",
    "        # if log_writer is not None:\n",
    "        #     log_writer.set_step(epoch * num_training_steps_per_epoch * args.update_freq)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, data_loader_train, optimizer, device, task_handler, epoch, \n",
    "            epoch * num_training_steps_per_epoch, lr_schedule_values, loss_scaler, \n",
    "            args.clip_grad, args.update_freq, model_ema, log_writer, args.task, mixup_fn,\n",
    "        )\n",
    "        if args.output_dir and args.save_ckpt:\n",
    "            if (epoch + 1) % args.save_ckpt_freq == 0 or epoch + 1 == args.epochs:\n",
    "                utils.save_model(\n",
    "                    args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                    loss_scaler=loss_scaler, epoch=epoch, model_ema=model_ema)\n",
    "        if data_loader_val is not None:\n",
    "            if args.task not in [\"coco_captioning\", \"nocaps\", \"sarcasm\"]:\n",
    "                test_stats, task_key = evaluate(data_loader_val, model, device, task_handler)\n",
    "            elif args.task == \"sarcasm\":\n",
    "                predictions = evaluate(data_loader_val, model, device, task_handler)\n",
    "                logits_all, labels_all, img_id_all = None, None, None\n",
    "                for b in predictions:\n",
    "                    if logits_all is None:\n",
    "                        logits_all = [b['logit']]\n",
    "                        labels_all = [b['label']]\n",
    "                        img_id_all = [b['image_id']]\n",
    "                    else:\n",
    "                        logits_all += [b['logit']]\n",
    "                        labels_all += [b['label']]\n",
    "                        img_id_all += [b['image_id']]\n",
    "                \n",
    "                logits_all = np.array(logits_all)\n",
    "                predicion_all = np.argmax(logits_all, axis=1)\n",
    "\n",
    "                f1 = metrics.f1_score(labels_all, predicion_all)\n",
    "                precision = metrics.precision_score(labels_all, predicion_all)\n",
    "                recall = metrics.recall_score(labels_all, predicion_all)\n",
    "                acc = metrics.accuracy_score(labels_all, predicion_all)\n",
    "                test_stats = {\n",
    "                    \"acc\": acc,\n",
    "                    \"f1\": f1,\n",
    "                    \"recall\": recall,\n",
    "                    \"precision\": precision,\n",
    "                }\n",
    "                logger.info(f\"acc {acc:4f} precision {precision:4f} recall {recall:4f} f1 {f1:4f}\")\n",
    "                task_key = \"acc\"\n",
    "            else:\n",
    "                predictions, _ = evaluate(data_loader_val, model, device, task_handler)\n",
    "                prediction_file = utils.dump_predictions(args, predictions, f\"{args.task}_val_e{epoch}\")\n",
    "                result_file = os.path.join(args.output_dir, f\"{args.task}_result_val_e{epoch}.json\")\n",
    "                task_key = \"CIDEr\"\n",
    "                if utils.is_main_process():\n",
    "                    test_stats = utils.coco_caption_eval(args.output_dir, prediction_file, \"{}_val\".format(args.task))\n",
    "                    utils.write_result_to_jsonl(test_stats, result_file)\n",
    "                torch.distributed.barrier()\n",
    "                if not utils.is_main_process():\n",
    "                    test_stats = utils.read_result_from_jsonl(result_file)\n",
    "\n",
    "            logger.info(f\"Performance of the network on the {len(data_loader_val.dataset)} val images: {test_stats[task_key]:.4f}%\")\n",
    "            if max_accuracy < test_stats[task_key]:\n",
    "                max_accuracy = test_stats[task_key]\n",
    "                if args.output_dir and args.save_ckpt:\n",
    "                    utils.save_model(\n",
    "                        args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                        loss_scaler=loss_scaler, epoch=\"best\", model_ema=model_ema)\n",
    "\n",
    "            logger.info(f'Max performance: {max_accuracy:.4f}')\n",
    "            if log_writer is not None:\n",
    "                # log_writer.update(acc=test_stats[task_key], head=\"perf\", step=epoch)\n",
    "                log_writer.info(f'epoch:{epoch} test_stats:{test_stats}')\n",
    "            \n",
    "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        **{f'val_{k}': v for k, v in test_stats.items()},\n",
    "                        'epoch': epoch,\n",
    "                        'n_parameters': n_parameters}\n",
    "        else:\n",
    "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                         # **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                         'epoch': epoch,\n",
    "                         'n_parameters': n_parameters}\n",
    "\n",
    "        if args.output_dir and utils.is_main_process():\n",
    "            logger.info(log_stats)\n",
    "            # if log_writer is not None:\n",
    "            #     log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    logger.info('Training time {}'.format(total_time_str))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opts, ds_init = get_args()\n",
    "    if opts.output_dir:\n",
    "        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    main(opts, ds_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
